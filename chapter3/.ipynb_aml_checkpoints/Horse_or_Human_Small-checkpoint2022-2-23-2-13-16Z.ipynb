{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "id": "zX4Kg8DUTKWO",
        "gather": {
          "logged": 1648001228598
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "id": "ioLbtB3uGKPX",
        "gather": {
          "logged": 1648001228911
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip \\\n",
        "    -O /tmp/horse-or-human.zip\n",
        "\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip \\\n",
        "    -O /tmp/validation-horse-or-human.zip"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "--2022-03-23 02:07:07--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip\nResolving storage.googleapis.com (storage.googleapis.com)... 172.217.0.48, 142.250.188.208, 172.253.115.128, ...\nConnecting to storage.googleapis.com (storage.googleapis.com)|172.217.0.48|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 149574867 (143M) [application/zip]\nSaving to: ‘/tmp/horse-or-human.zip’\n\n/tmp/horse-or-human 100%[===================>] 142.65M   133MB/s    in 1.1s    \n\n2022-03-23 02:07:08 (133 MB/s) - ‘/tmp/horse-or-human.zip’ saved [149574867/149574867]\n\n--2022-03-23 02:07:09--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip\nResolving storage.googleapis.com (storage.googleapis.com)... 172.217.13.80, 172.253.63.128, 142.251.45.112, ...\nConnecting to storage.googleapis.com (storage.googleapis.com)|172.217.13.80|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 11480187 (11M) [application/zip]\nSaving to: ‘/tmp/validation-horse-or-human.zip’\n\n/tmp/validation-hor 100%[===================>]  10.95M  66.1MB/s    in 0.2s    \n\n2022-03-23 02:07:09 (66.1 MB/s) - ‘/tmp/validation-horse-or-human.zip’ saved [11480187/11480187]\n\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "id": "RXZT2UsyIVe_",
        "gather": {
          "logged": 1648001230764
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = '/tmp/horse-or-human.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/horse-or-human')\n",
        "local_zip = '/tmp/validation-horse-or-human.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/validation-horse-or-human')\n",
        "zip_ref.close()\n",
        "\n",
        "# Directory with our training horse pictures\n",
        "train_horse_dir = os.path.join('/tmp/horse-or-human/horses')\n",
        "\n",
        "# Directory with our training human pictures\n",
        "train_human_dir = os.path.join('/tmp/horse-or-human/humans')\n",
        "\n",
        "# Directory with our training horse pictures\n",
        "validation_horse_dir = os.path.join('/tmp/validation-horse-or-human/horses')\n",
        "\n",
        "# Directory with our training human pictures\n",
        "validation_human_dir = os.path.join('/tmp/validation-horse-or-human/humans')\n",
        "\n",
        "train_horse_names = os.listdir('/tmp/horse-or-human/horses')\n",
        "print(train_horse_names[:10])\n",
        "\n",
        "train_human_names = os.listdir('/tmp/horse-or-human/humans')\n",
        "print(train_human_names[:10])\n",
        "\n",
        "validation_horse_hames = os.listdir('/tmp/validation-horse-or-human/horses')\n",
        "print(validation_horse_hames[:10])\n",
        "\n",
        "validation_human_names = os.listdir('/tmp/validation-horse-or-human/humans')\n",
        "print(validation_human_names[:10])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "['horse43-1.png', 'horse22-9.png', 'horse32-6.png', 'horse18-4.png', 'horse07-8.png', 'horse05-7.png', 'horse44-1.png', 'horse30-2.png', 'horse26-2.png', 'horse41-1.png']\n['human09-24.png', 'human17-17.png', 'human04-17.png', 'human01-27.png', 'human10-20.png', 'human16-13.png', 'human12-19.png', 'human17-11.png', 'human15-11.png', 'human02-19.png']\n['horse4-541.png', 'horse6-089.png', 'horse2-218.png', 'horse6-403.png', 'horse2-314.png', 'horse3-011.png', 'horse5-192.png', 'horse1-384.png', 'horse3-521.png', 'horse4-072.png']\n['valhuman02-22.png', 'valhuman01-02.png', 'valhuman05-10.png', 'valhuman05-13.png', 'valhuman04-20.png', 'valhuman01-05.png', 'valhuman02-07.png', 'valhuman05-21.png', 'valhuman02-08.png', 'valhuman04-09.png']\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "id": "PLy3pthUS0D2",
        "gather": {
          "logged": 1648001231716
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "id": "qvfZg3LQbD-5",
        "gather": {
          "logged": 1648001254411
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    # Note the input shape is the desired size of the image 150x150 with 3 bytes color\n",
        "    # This is the first convolution\n",
        "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    # The second convolution\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The third convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The fourth convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # Flatten the results to feed into a DNN\n",
        "    tf.keras.layers.Flatten(),\n",
        "    # 512 neuron hidden layer\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('horses') and 1 for the other ('humans')\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "id": "PixZ2s5QbYQ3",
        "gather": {
          "logged": 1648001255981
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=RMSprop(lr=0.001),\n",
        "              metrics=['acc'])"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "id": "8DHWhFP_uhq3",
        "gather": {
          "logged": 1648001256165
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# All images will be augmented\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        "\n",
        "# Flow training images in batches of 128 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/tmp/horse-or-human/',  # This is the source directory for training images\n",
        "        target_size=(150, 150),  # All images will be resized to 150x150\n",
        "        batch_size=128,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='binary')\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1/255)\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "        '/tmp/validation-horse-or-human',\n",
        "        target_size=(150, 150),\n",
        "        class_mode='binary')\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Found 1027 images belonging to 2 classes.\nFound 256 images belonging to 2 classes.\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "id": "ClebU9NJg99G",
        "gather": {
          "logged": 1648001256368
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "      train_generator,\n",
        "      steps_per_epoch=8,  \n",
        "      epochs=15,\n",
        "      verbose=1,\n",
        "      validation_data=validation_generator)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "WARNING:tensorflow:sample_weight modes were coerced from\n  ...\n    to  \n  ['...']\nWARNING:tensorflow:sample_weight modes were coerced from\n  ...\n    to  \n  ['...']\nTrain for 8 steps, validate for 8 steps\nEpoch 1/15\n8/8 [==============================] - 18s 2s/step - loss: 0.9011 - acc: 0.5449 - val_loss: 0.8517 - val_acc: 0.5000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 2/15\n8/8 [==============================] - 15s 2s/step - loss: 0.6811 - acc: 0.5973 - val_loss: 0.8774 - val_acc: 0.5000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 3/15\n8/8 [==============================] - 15s 2s/step - loss: 0.6590 - acc: 0.6496 - val_loss: 0.8881 - val_acc: 0.5000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 4/15\n8/8 [==============================] - 15s 2s/step - loss: 0.5683 - acc: 0.6841 - val_loss: 0.7106 - val_acc: 0.5938\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 5/15\n8/8 [==============================] - 15s 2s/step - loss: 0.6045 - acc: 0.6541 - val_loss: 0.6599 - val_acc: 0.6953\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 6/15\n8/8 [==============================] - 15s 2s/step - loss: 0.4636 - acc: 0.7775 - val_loss: 0.6567 - val_acc: 0.7109\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 7/15\n8/8 [==============================] - 15s 2s/step - loss: 0.6035 - acc: 0.7119 - val_loss: 0.7093 - val_acc: 0.5000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 8/15\n8/8 [==============================] - 15s 2s/step - loss: 0.4407 - acc: 0.7631 - val_loss: 1.1909 - val_acc: 0.5117\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 9/15\n8/8 [==============================] - 15s 2s/step - loss: 0.4857 - acc: 0.7486 - val_loss: 0.7922 - val_acc: 0.5078\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 10/15\n8/8 [==============================] - 16s 2s/step - loss: 0.4793 - acc: 0.7881 - val_loss: 0.4967 - val_acc: 0.8125\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 11/15\n8/8 [==============================] - 15s 2s/step - loss: 0.3966 - acc: 0.8265 - val_loss: 0.5306 - val_acc: 0.7227\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 12/15\n8/8 [==============================] - 15s 2s/step - loss: 0.3337 - acc: 0.8331 - val_loss: 2.8178 - val_acc: 0.5039\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 13/15\n8/8 [==============================] - 16s 2s/step - loss: 0.3780 - acc: 0.8223 - val_loss: 1.2282 - val_acc: 0.6133\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 14/15\n8/8 [==============================] - 15s 2s/step - loss: 0.4167 - acc: 0.7898 - val_loss: 1.1620 - val_acc: 0.6289\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 15/15\n8/8 [==============================] - 15s 2s/step - loss: 0.2647 - acc: 0.8832 - val_loss: 0.4109 - val_acc: 0.8008\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "id": "Fb1_lgobv81m",
        "gather": {
          "logged": 1648001489273
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Running the Model\n",
        "\n",
        "Let's now take a look at actually running a prediction using the model. This code will allow you to choose 1 or more files from your file system, it will then upload them, and run them through the model, giving an indication of whether the object is a horse or a human."
      ],
      "metadata": {
        "id": "o6vSHzPR2ghH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from google.colab import files\n",
        "from keras.preprocessing import image\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        " \n",
        "  # predicting images\n",
        "  path = '/content/' + fn\n",
        "  img = image.load_img(path, target_size=(150, 150))\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "\n",
        "  image_tensor = np.vstack([x])\n",
        "  classes = model.predict(image_tensor)\n",
        "  print(classes)\n",
        "  print(classes[0])\n",
        "  if classes[0]>0.5:\n",
        "    print(fn + \" is a human\")\n",
        "  else:\n",
        "    print(fn + \" is a horse\")\n",
        " "
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-e40ac4652996>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
          ]
        }
      ],
      "execution_count": 10,
      "metadata": {
        "id": "DoWp43WxJDNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory with our training horse pictures\n",
        "train_horse_dir = os.path.join('/tmp/horse-or-human/horses')\n",
        "\n",
        "# Directory with our training human pictures\n",
        "train_human_dir = os.path.join('/tmp/horse-or-human/humans')\n",
        "\n",
        "# Directory with our training horse pictures\n",
        "validation_horse_dir = os.path.join('/tmp/validation-horse-or-human/horses')\n",
        "\n",
        "# Directory with our training human pictures\n",
        "validation_human_dir = os.path.join('/tmp/validation-horse-or-human/humans')\n",
        "\n",
        "train_horse_names = os.listdir('/tmp/horse-or-human/horses')\n",
        "print(train_horse_names[:10])\n",
        "\n",
        "train_human_names = os.listdir('/tmp/horse-or-human/humans')\n",
        "print(train_human_names[:10])\n",
        "\n",
        "validation_horse_hames = os.listdir('/tmp/validation-horse-or-human/horses')\n",
        "print(validation_horse_hames[:10])\n",
        "\n",
        "validation_human_names = os.listdir('/tmp/validation-horse-or-human/humans')\n",
        "print(validation_human_names[:10])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "lVDyNR0d1JMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "\n",
        "# Let's define a new Model that will take an image as input, and will output\n",
        "# intermediate representations for all layers in the previous model after\n",
        "# the first.\n",
        "successive_outputs = [layer.output for layer in model.layers[1:]]\n",
        "#visualization_model = Model(img_input, successive_outputs)\n",
        "visualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)\n",
        "# Let's prepare a random input image from the training set.\n",
        "horse_img_files = [os.path.join(train_horse_dir, f) for f in train_horse_names]\n",
        "human_img_files = [os.path.join(train_human_dir, f) for f in train_human_names]\n",
        "img_path = random.choice(horse_img_files + human_img_files)\n",
        "\n",
        "img = load_img(img_path, target_size=(150, 150))  # this is a PIL image\n",
        "x = img_to_array(img)  # Numpy array with shape (150, 150, 3)\n",
        "x = x.reshape((1,) + x.shape)  # Numpy array with shape (1, 150, 150, 3)\n",
        "\n",
        "# Rescale by 1/255\n",
        "x /= 255\n",
        "\n",
        "# Let's run our image through our network, thus obtaining all\n",
        "# intermediate representations for this image.\n",
        "successive_feature_maps = visualization_model.predict(x)\n",
        "\n",
        "# These are the names of the layers, so can have them as part of our plot\n",
        "layer_names = [layer.name for layer in model.layers]\n",
        "\n",
        "# Now let's display our representations\n",
        "for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n",
        "  if len(feature_map.shape) == 4:\n",
        "    # Just do this for the conv / maxpool layers, not the fully-connected layers\n",
        "    n_features = feature_map.shape[-1]  # number of features in feature map\n",
        "    # The feature map has shape (1, size, size, n_features)\n",
        "    size = feature_map.shape[1]\n",
        "    # We will tile our images in this matrix\n",
        "    display_grid = np.zeros((size, size * n_features))\n",
        "    for i in range(n_features):\n",
        "      # Postprocess the feature to make it visually palatable\n",
        "      x = feature_map[0, :, :, i]\n",
        "      x -= x.mean()\n",
        "      x /= x.std()\n",
        "      x *= 64\n",
        "      x += 128\n",
        "      x = np.clip(x, 0, 255).astype('uint8')\n",
        "      # We'll tile each filter into this big horizontal grid\n",
        "      display_grid[:, i * size : (i + 1) * size] = x\n",
        "    # Display the grid\n",
        "    scale = 20. / n_features\n",
        "    plt.figure(figsize=(scale * n_features, scale))\n",
        "    plt.title(layer_name)\n",
        "    plt.grid(False)\n",
        "    plt.imshow(display_grid, aspect='auto', cmap='viridis')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "h-SUwB0bzVvc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clean Up\n",
        "\n",
        "Before running the next exercise, run the following cell to terminate the kernel and free memory resources:"
      ],
      "metadata": {
        "id": "j4IBgYCYooGD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, signal\n",
        "os.kill(os.getpid(), signal.SIGKILL)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "651IgjLyo-Jx"
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "Horse-or-Human-Small.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}